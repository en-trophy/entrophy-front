import { useEffect, useRef, useState } from 'react';
import Webcam from 'react-webcam';
import { Holistic, HAND_CONNECTIONS, POSE_CONNECTIONS } from '@mediapipe/holistic';
import type { Results } from '@mediapipe/holistic';
import { Camera as MediaPipeCamera } from '@mediapipe/camera_utils';
import { drawConnectors } from '@mediapipe/drawing_utils';
import { aiApi } from '../services/api';
import type { Pose } from '../types';
import './Camera.css';

interface CameraProps {
  targetPose: Pose | null;
  lessonId: string;
  onScoreUpdate?: (score: number) => void;
  onSuccess?: () => void;
  onFeedback?: (feedback: string, score: number) => void;
  isRunning?: boolean;
}

export default function Camera({ lessonId, onScoreUpdate, onSuccess, onFeedback, isRunning = true }: CameraProps) {
  const webcamRef = useRef<Webcam>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const holisticRef = useRef<Holistic | null>(null);
  const stillStartTime = useRef<number | null>(null);
  const isRunningRef = useRef<boolean>(isRunning); // isRunningì„ refë¡œ ì €ì¥
  const [isWebcamReady, setIsWebcamReady] = useState(false);
  const [countdown, setCountdown] = useState(5000);

  // isRunning propì´ ë³€ê²½ë  ë•Œ ref ì—…ë°ì´íŠ¸
  useEffect(() => {
    console.log('ğŸ” Camera isRunning prop changed to:', isRunning);
    isRunningRef.current = isRunning;
  }, [isRunning]);

  // MediaPipe Holistic ì´ˆê¸°í™”
  useEffect(() => {
    holisticRef.current = new Holistic({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`,
    });

    holisticRef.current.setOptions({
      modelComplexity: 1,
      smoothLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5,
    });

    holisticRef.current.onResults(onHolisticResults);

    return () => {
      holisticRef.current?.close();
    };
  }, [lessonId]);

  // MediaPipe ê²°ê³¼ ì²˜ë¦¬ (5ì´ˆë§ˆë‹¤ ìë™ ì „ì†¡)
  const onHolisticResults = async (results: Results) => {
    // Canvasì— skeleton ê·¸ë¦¬ê¸° (í•­ìƒ í‘œì‹œ)
    drawSkeletonOnCanvas(results);

    console.log('â±ï¸ onHolisticResults called - isRunning:', isRunningRef.current, 'countdown:', countdown);

    // isRunningì´ falseë©´ ê²€ì‚¬ ì¤‘ì§€
    if (!isRunningRef.current) {
      stillStartTime.current = null;
      setCountdown(5000);
      return;
    }

    const currentTime = Date.now();

    // ì²˜ìŒ ì‹œì‘í•  ë•Œ
    if (stillStartTime.current === null) {
      stillStartTime.current = currentTime;
      console.log('ğŸ¬ Timer started at:', currentTime);
    }

    const elapsed = currentTime - stillStartTime.current;
    const remaining = Math.max(0, 5000 - elapsed);
    setCountdown(remaining);

    // 5ì´ˆ ê²½ê³¼í•˜ë©´ AI ì„œë²„ë¡œ ì „ì†¡
    if (elapsed >= 5000) {
      console.log('[AI] Sending to server...');
      await sendFeedback(results);
      // ì „ì†¡ í›„ íƒ€ì´ë¨¸ ë¦¬ì…‹
      stillStartTime.current = null;
    }
  };

  // ì›¹ìº  ì´ë¯¸ì§€ë¥¼ ìº¡ì²˜í•˜ëŠ” í•¨ìˆ˜
  const captureWebcamImage = async (): Promise<Blob | null> => {
    const video = webcamRef.current?.video;
    if (!video) return null;

    // ì„ì‹œ canvas ìƒì„±
    const captureCanvas = document.createElement('canvas');
    captureCanvas.width = video.videoWidth;
    captureCanvas.height = video.videoHeight;

    const ctx = captureCanvas.getContext('2d');
    if (!ctx) return null;

    // ë¹„ë””ì˜¤ í”„ë ˆì„ì„ canvasì— ê·¸ë¦¬ê¸° (ê±°ìš¸ ëª¨ë“œ ì œê±° - AI ì„œë²„ìš©)
    ctx.drawImage(video, 0, 0);

    // canvasë¥¼ Blobìœ¼ë¡œ ë³€í™˜
    return new Promise((resolve) => {
      captureCanvas.toBlob((blob) => {
        resolve(blob);
      }, 'image/jpeg', 0.95);
    });
  };

  // AI ì„œë²„ë¡œ ì´ë¯¸ì§€ë¥¼ ë³´ë‚´ëŠ” í•¨ìˆ˜
  const sendFeedback = async (results: Results) => {
    if (!lessonId) return;

    const numericLessonId = parseInt(lessonId, 10);
    if (isNaN(numericLessonId)) {
      console.error('Invalid lessonId:', lessonId);
      return;
    }

    try {
      // ì›¹ìº  ì´ë¯¸ì§€ ìº¡ì²˜
      const imageBlob = await captureWebcamImage();
      if (!imageBlob) {
        console.error('Failed to capture webcam image');
        return;
      }

      console.log('ğŸ“· Captured image, size:', imageBlob.size, 'bytes');

      // AI ì„œë²„ë¡œ ì´ë¯¸ì§€ ì „ì†¡
      const data = await aiApi.sendFeedback(numericLessonId, imageBlob);
      console.log('AI Server response:', data);

      // ì„œë²„ì—ì„œ ë°›ì€ scoreë¥¼ 0~100ìœ¼ë¡œ ë³€í™˜ (ì„œë²„ëŠ” 0~1 ì‚¬ì´ë¡œ ë³´ëƒ„)
      const scorePercent = Math.round(data.score * 100);
      if (data.score !== undefined) {
        onScoreUpdate?.(scorePercent);
      }

      // 100ì ì´ê±°ë‚˜ ì„±ê³µ íŒì •ì´ë©´ ì„±ê³µ ëª¨ë‹¬ í‘œì‹œ
      if (scorePercent === 100 || data.isCorrect) {
        console.log('Success! Sign language is correct.');
        onSuccess?.();
      } else {
        // 100ì  ë¯¸ë§Œì¼ ë•Œë§Œ í”¼ë“œë°± ëª¨ë‹¬ í‘œì‹œ
        if (data.feedback) {
          onFeedback?.(data.feedback, scorePercent);
        }
      }
    } catch (error) {
      console.error('Failed to send feedback to AI server:', error);
    }
  };

  // Canvasì— skeleton ê·¸ë¦¬ê¸°
  const drawSkeletonOnCanvas = (results: Results) => {
    const canvas = canvasRef.current;
    const video = webcamRef.current?.video;
    if (!canvas || !video) return;

    const ctx = canvas.getContext('2d');
    if (!ctx) return;

    // Canvas í¬ê¸° ì„¤ì •
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    // Canvas ì´ˆê¸°í™”
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    // MediaPipe skeleton ê·¸ë¦¬ê¸°
    ctx.save();
    ctx.translate(canvas.width, 0);
    ctx.scale(-1, 1);

    drawConnectors(ctx, results.poseLandmarks, POSE_CONNECTIONS, { color: '#00FF00', lineWidth: 2 });
    drawConnectors(ctx, results.leftHandLandmarks, HAND_CONNECTIONS, { color: '#FF0000', lineWidth: 2 });
    drawConnectors(ctx, results.rightHandLandmarks, HAND_CONNECTIONS, { color: '#0000FF', lineWidth: 2 });

    ctx.restore();
  };

  // ì›¹ìº  ì¤€ë¹„ ì™„ë£Œ í›„ MediaPipe Camera ì‹œì‘
  useEffect(() => {
    const video = webcamRef.current?.video;
    if (!video || !holisticRef.current) return;

    const camera = new MediaPipeCamera(video, {
      onFrame: async () => {
        if (webcamRef.current?.video && holisticRef.current) {
          await holisticRef.current.send({ image: webcamRef.current.video });
        }
      },
      width: 640,
      height: 480,
    });

    camera.start();
    setIsWebcamReady(true);

    return () => {
      camera.stop();
    };
  }, []);

  return (
    <div className="camera-container">
      <Webcam
        ref={webcamRef}
        mirrored={true}
        className="camera-video"
        videoConstraints={{ width: 640, height: 480 }}
      />
      <canvas ref={canvasRef} className="camera-canvas" />

      {/* ìƒíƒœ í‘œì‹œ */}
      {isWebcamReady && (
        <div className={`camera-status ${isRunning ? '' : 'paused'}`}>
          {isRunning ? 'Checking...' : 'Paused'}
        </div>
      )}

      {/* ì¹´ìš´íŠ¸ë‹¤ìš´ í‘œì‹œ */}
      {isWebcamReady && isRunning && (
        <div className="camera-countdown">
          {countdown === 0 ? 'Sending...' : `Next check: ${(countdown / 1000).toFixed(1)}s`}
        </div>
      )}

      {!isWebcamReady && (
        <div className="camera-loading">Loading webcam...</div>
      )}
    </div>
  );
}
