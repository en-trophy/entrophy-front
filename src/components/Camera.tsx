import { useEffect, useRef, useState } from 'react';
import Webcam from 'react-webcam';
import { Holistic, HAND_CONNECTIONS, POSE_CONNECTIONS } from '@mediapipe/holistic';
import type { Results } from '@mediapipe/holistic';
import { Camera as MediaPipeCamera } from '@mediapipe/camera_utils';
import { drawConnectors } from '@mediapipe/drawing_utils';
import { aiApi } from '../services/api';
import type { Pose } from '../types';
import './Camera.css';

interface FrameCaptureConfig {
  frameCount: number;      // ìº¡ì²˜í•  í”„ë ˆì„ ìˆ˜
  intervalMs: number;      // í”„ë ˆì„ ê°„ ê°„ê²© (ë°€ë¦¬ì´ˆ)
}

// í•˜ë“œì½”ë”©ëœ ì„¤ì • (í–¥í›„ ë°±ì—”ë“œì—ì„œ ë°›ì•„ì˜¬ ì˜ˆì •)
const LESSON_FRAME_CONFIG: Record<number, FrameCaptureConfig> = {
  4: { frameCount: 2, intervalMs: 500 }, // "thank you" - 2í”„ë ˆì„, 0.5ì´ˆ ê°„ê²©
};

const DEFAULT_FRAME_CONFIG: FrameCaptureConfig = {
  frameCount: 1,
  intervalMs: 0,
};

const getFrameConfig = (lessonId: number): FrameCaptureConfig => {
  return LESSON_FRAME_CONFIG[lessonId] || DEFAULT_FRAME_CONFIG;
};

interface CameraProps {
  targetPose: Pose | null;
  lessonId: string;
  onScoreUpdate?: (score: number) => void;
  onSuccess?: () => void;
  onFeedback?: (feedback: string, score: number) => void;
  isRunning?: boolean;
  onAnalyzingChange?: (isAnalyzing: boolean) => void;
}

export default function Camera({ lessonId, onScoreUpdate, onSuccess, onFeedback, isRunning = true, onAnalyzingChange }: CameraProps) {
  const webcamRef = useRef<Webcam>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const holisticRef = useRef<Holistic | null>(null);
  const stillStartTime = useRef<number | null>(null);
  const isRunningRef = useRef<boolean>(isRunning); // isRunningì„ refë¡œ ì €ì¥
  const [isWebcamReady, setIsWebcamReady] = useState(false);
  const [countdown, setCountdown] = useState(5000);
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [capturedFrames, setCapturedFrames] = useState<Blob[]>([]);
  const [currentFrame, setCurrentFrame] = useState(0);
  const [totalFrames, setTotalFrames] = useState(1);
  const [showAnalyzingOverlay, setShowAnalyzingOverlay] = useState(true);

  // isRunning propì´ ë³€ê²½ë  ë•Œ ref ì—…ë°ì´íŠ¸
  useEffect(() => {
    console.log('ğŸ” Camera isRunning prop changed to:', isRunning);
    isRunningRef.current = isRunning;
  }, [isRunning]);

  // isAnalyzingì´ ë³€ê²½ë  ë•Œ ë¶€ëª¨ ì»´í¬ë„ŒíŠ¸ì— ì•Œë¦¼
  useEffect(() => {
    onAnalyzingChange?.(isAnalyzing);
  }, [isAnalyzing, onAnalyzingChange]);

  // MediaPipe Holistic ì´ˆê¸°í™”
  useEffect(() => {
    holisticRef.current = new Holistic({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`,
    });

    holisticRef.current.setOptions({
      modelComplexity: 1,
      smoothLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5,
    });

    holisticRef.current.onResults(onHolisticResults);

    return () => {
      holisticRef.current?.close();
    };
  }, [lessonId]);

  // MediaPipe ê²°ê³¼ ì²˜ë¦¬ (5ì´ˆë§ˆë‹¤ ìë™ ì „ì†¡)
  const onHolisticResults = async (results: Results) => {
    // Canvasì— skeleton ê·¸ë¦¬ê¸° (í•­ìƒ í‘œì‹œ)
    drawSkeletonOnCanvas(results);

    console.log('â±ï¸ onHolisticResults called - isRunning:', isRunningRef.current, 'countdown:', countdown);

    // isRunningì´ falseë©´ ê²€ì‚¬ ì¤‘ì§€
    if (!isRunningRef.current) {
      stillStartTime.current = null;
      setCountdown(5000);
      return;
    }

    const currentTime = Date.now();

    // ì²˜ìŒ ì‹œì‘í•  ë•Œ
    if (stillStartTime.current === null) {
      stillStartTime.current = currentTime;
      console.log('ğŸ¬ Timer started at:', currentTime);
    }

    const elapsed = currentTime - stillStartTime.current;
    const remaining = Math.max(0, 5000 - elapsed);
    setCountdown(remaining);

    // 5ì´ˆ ê²½ê³¼í•˜ë©´ ìº¡ì²˜ ì‹œì‘
    if (elapsed >= 5000) {
      console.log('[AI] 5 seconds elapsed, starting capture sequence...');
      // íƒ€ì´ë¨¸ ë©ˆì¶¤ & analyzing ì‹œì‘
      stillStartTime.current = null;
      isRunningRef.current = false; // íƒ€ì´ë¨¸ ì •ì§€
      setIsAnalyzing(true);

      // ë ˆìŠ¨ë³„ í”„ë ˆì„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
      const numericLessonId = parseInt(lessonId, 10);
      const config = getFrameConfig(numericLessonId);

      if (config.frameCount === 1) {
        // ë‹¨ì¼ í”„ë ˆì„ - ê¸°ì¡´ ë°©ì‹
        await sendFeedback();
      } else {
        // ë©€í‹° í”„ë ˆì„ ìº¡ì²˜
        await captureMultipleFrames(config);
      }
    }
  };

  // ì›¹ìº  ì´ë¯¸ì§€ë¥¼ ìº¡ì²˜í•˜ëŠ” í•¨ìˆ˜
  const captureWebcamImage = async (): Promise<Blob | null> => {
    const video = webcamRef.current?.video;
    if (!video) return null;

    // ì„ì‹œ canvas ìƒì„±
    const captureCanvas = document.createElement('canvas');
    captureCanvas.width = video.videoWidth;
    captureCanvas.height = video.videoHeight;

    const ctx = captureCanvas.getContext('2d');
    if (!ctx) return null;

    // ë¹„ë””ì˜¤ í”„ë ˆì„ì„ canvasì— ê·¸ë¦¬ê¸° (ê±°ìš¸ ëª¨ë“œ ì œê±° - AI ì„œë²„ìš©)
    ctx.drawImage(video, 0, 0);

    // canvasë¥¼ Blobìœ¼ë¡œ ë³€í™˜
    return new Promise((resolve) => {
      captureCanvas.toBlob((blob) => {
        resolve(blob);
      }, 'image/jpeg', 0.95);
    });
  };

  // ì—¬ëŸ¬ í”„ë ˆì„ì„ ìˆœì°¨ì ìœ¼ë¡œ ìº¡ì²˜
  const captureMultipleFrames = async (config: FrameCaptureConfig) => {
    const frames: Blob[] = [];
    const { frameCount, intervalMs } = config;

    setTotalFrames(frameCount);
    setShowAnalyzingOverlay(false); // ì´ˆê¸°ì—ëŠ” overlay ìˆ¨ê¹€

    const FLASH_DURATION_MS = 150; // Overlay í‘œì‹œ ì‹œê°„

    console.log(`[Multi-Frame] Starting capture: ${frameCount} frames, ${intervalMs}ms interval`);

    for (let i = 0; i < frameCount; i++) {
      setCurrentFrame(i + 1);

      // Overlay í‘œì‹œ ("Capturing frame X/Y...")
      setShowAnalyzingOverlay(true);
      await new Promise(resolve => setTimeout(resolve, FLASH_DURATION_MS));

      // í”„ë ˆì„ ìº¡ì²˜
      const frameBlob = await captureWebcamImage();
      if (!frameBlob) {
        console.error(`[Multi-Frame] Failed to capture frame ${i + 1}`);
        setShowAnalyzingOverlay(false);
        setIsAnalyzing(false);
        onFeedback?.(`Failed to capture frame ${i + 1}. Please check your camera and try again.`, 0);
        return;
      }

      frames.push(frameBlob);
      console.log(`[Multi-Frame] Frame ${i + 1} captured, size: ${frameBlob.size} bytes`);

      // Overlay ìˆ¨ê¹€ (ì›¹ìº  í™”ë©´ ë³´ì„)
      setShowAnalyzingOverlay(false);

      // ë‹¤ìŒ í”„ë ˆì„ê¹Œì§€ ëŒ€ê¸° (ë§ˆì§€ë§‰ í”„ë ˆì„ ì œì™¸)
      if (i < frameCount - 1) {
        const remainingInterval = intervalMs - FLASH_DURATION_MS;
        await new Promise(resolve => setTimeout(resolve, remainingInterval));
      }
    }

    setCapturedFrames(frames);
    console.log(`[Multi-Frame] All ${frameCount} frames captured, sending to API...`);

    // ëª¨ë“  í”„ë ˆì„ ìº¡ì²˜ ì™„ë£Œ - Analyzing í‘œì‹œ
    setShowAnalyzingOverlay(true);
    setCurrentFrame(0); // currentFrameì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ "Analyzing..." í…ìŠ¤íŠ¸ í‘œì‹œ

    // ëª¨ë“  í”„ë ˆì„ì„ APIë¡œ ì „ì†¡
    await sendFeedback(frames);
  };

  // AI ì„œë²„ë¡œ ì´ë¯¸ì§€ë¥¼ ë³´ë‚´ëŠ” í•¨ìˆ˜
  const sendFeedback = async (frames?: Blob[]) => {
    if (!lessonId) return;

    const numericLessonId = parseInt(lessonId, 10);
    if (isNaN(numericLessonId)) {
      console.error('Invalid lessonId:', lessonId);
      return;
    }

    try {
      // ë‹¨ì¼ í”„ë ˆì„ ë˜ëŠ” ë©€í‹° í”„ë ˆì„ ì²˜ë¦¬
      const imagesToSend = frames || [await captureWebcamImage()];

      // ëª¨ë“  í”„ë ˆì„ ìœ íš¨ì„± ê²€ì‚¬
      if (imagesToSend.some(blob => !blob)) {
        console.error('Failed to capture one or more images');
        setIsAnalyzing(false);
        return;
      }

      console.log(`ğŸ“· Sending ${imagesToSend.length} frame(s) to AI server`);

      // AI ì„œë²„ë¡œ ì „ì†¡
      const data = await aiApi.sendFeedback(numericLessonId, imagesToSend);
      console.log('AI Server response:', data);

      // ì„œë²„ì—ì„œ ë°›ì€ scoreë¥¼ 0~100ìœ¼ë¡œ ë³€í™˜ (ì„œë²„ëŠ” 0~1 ì‚¬ì´ë¡œ ë³´ëƒ„)
      const scorePercent = Math.round(data.score * 100);
      if (data.score !== undefined) {
        onScoreUpdate?.(scorePercent);
      }

      // 100ì ì´ê±°ë‚˜ ì„±ê³µ íŒì •ì´ë©´ ì„±ê³µ ëª¨ë‹¬ í‘œì‹œ
      if (scorePercent === 100 || data.isCorrect) {
        console.log('Success! Sign language is correct.');
        onSuccess?.();
        setIsAnalyzing(false); // ì„±ê³µ ì‹œ analyzing ì¢…ë£Œ
      } else {
        // 100ì  ë¯¸ë§Œì¼ ë•Œë§Œ í”¼ë“œë°± ëª¨ë‹¬ í‘œì‹œ
        if (data.feedback) {
          onFeedback?.(data.feedback, scorePercent);
          setIsAnalyzing(false); // í”¼ë“œë°± ì‹œ analyzing ì¢…ë£Œ
        }
      }
    } catch (error) {
      console.error('Failed to send feedback to AI server:', error);
      setIsAnalyzing(false); // ì—ëŸ¬ ì‹œì—ë„ analyzing ì¢…ë£Œ
    } finally {
      // ë©€í‹° í”„ë ˆì„ ìƒíƒœ ì´ˆê¸°í™”
      setCapturedFrames([]);
      setCurrentFrame(0);
      setTotalFrames(1);
      setShowAnalyzingOverlay(true); // ë‹¤ìŒ ì„¸ì…˜ì„ ìœ„í•´ ì´ˆê¸°ê°’ìœ¼ë¡œ ë¦¬ì…‹
    }
  };

  // Canvasì— skeleton ê·¸ë¦¬ê¸°
  const drawSkeletonOnCanvas = (results: Results) => {
    const canvas = canvasRef.current;
    const video = webcamRef.current?.video;
    if (!canvas || !video) return;

    const ctx = canvas.getContext('2d');
    if (!ctx) return;

    // Canvas í¬ê¸° ì„¤ì •
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    // Canvas ì´ˆê¸°í™”
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    // MediaPipe skeleton ê·¸ë¦¬ê¸°
    ctx.save();
    ctx.translate(canvas.width, 0);
    ctx.scale(-1, 1);

    drawConnectors(ctx, results.poseLandmarks, POSE_CONNECTIONS, { color: '#00FF00', lineWidth: 2 });
    drawConnectors(ctx, results.leftHandLandmarks, HAND_CONNECTIONS, { color: '#FF0000', lineWidth: 2 });
    drawConnectors(ctx, results.rightHandLandmarks, HAND_CONNECTIONS, { color: '#0000FF', lineWidth: 2 });

    ctx.restore();
  };

  // ì›¹ìº  ì¤€ë¹„ ì™„ë£Œ í›„ MediaPipe Camera ì‹œì‘
  useEffect(() => {
    const video = webcamRef.current?.video;
    if (!video || !holisticRef.current) return;

    const camera = new MediaPipeCamera(video, {
      onFrame: async () => {
        if (webcamRef.current?.video && holisticRef.current) {
          await holisticRef.current.send({ image: webcamRef.current.video });
        }
      },
      width: 640,
      height: 480,
    });

    camera.start();
    setIsWebcamReady(true);

    return () => {
      camera.stop();
    };
  }, []);

  return (
    <div className="camera-container">
      <Webcam
        ref={webcamRef}
        mirrored={true}
        className="camera-video"
        videoConstraints={{ width: 640, height: 480 }}
      />
      <canvas ref={canvasRef} className="camera-canvas" />

      {/* ìƒíƒœ í‘œì‹œ */}
      {isWebcamReady && (
        <div className={`camera-status ${isRunning ? '' : 'paused'}`}>
          {isRunning ? 'Checking...' : 'Paused'}
        </div>
      )}

      {/* ì¹´ìš´íŠ¸ë‹¤ìš´ í‘œì‹œ */}
      {isWebcamReady && isRunning && !isAnalyzing && (
        <div className="camera-countdown">
          Next check: {(countdown / 1000).toFixed(1)}s
        </div>
      )}

      {/* Analyzing Overlay */}
      {isAnalyzing && showAnalyzingOverlay && (
        <div className="camera-analyzing-overlay">
          <div className="analyzing-spinner"></div>
          <div className="analyzing-text">
            {totalFrames > 1 && currentFrame > 0 && currentFrame <= totalFrames
              ? `Capturing frame ${currentFrame}/${totalFrames}...`
              : 'Analyzing...'}
          </div>
        </div>
      )}

      {!isWebcamReady && (
        <div className="camera-loading">Loading webcam...</div>
      )}
    </div>
  );
}
